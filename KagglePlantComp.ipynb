{"cells":[{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"# Hunter Mitchell - 5/26/20 - Plant Pathology 2020 Competition Code\n\n# This code uses TPU and an ensemble of multiple deep learning models to classify the diseases on images of apple leaves\n\n\n\n!pip install -q efficientnet\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport cv2\n\nimport tensorflow.keras.layers as L\nimport efficientnet.tfkeras as efn\nfrom tensorflow.keras.applications import DenseNet201,InceptionResNetV2,Xception,ResNet50V2\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom kaggle_datasets import KaggleDatasets\n\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.utils.class_weight import compute_class_weight\n\n\n\n\n\n\n\n### SETTINGS ###\n\n# Image Size (Original images are 1365 x 2048)\nimg_size_x = 512\nimg_size_y = 512\n\n# Cross Validation Settings\nkfolding = False\nFOLDS = 5 \n\n# Splitting Settings\nTEST_SIZE = .15 \nSEED = 2020\n\n# How many different models to ensemble (up to 5)\nnumber_of_models = 3\n\n# Model Settings\nBATCH_SIZE = 8\nEPOCHS = 35\n\n\n\n\n\n\n\n### TPU Stuff\ndef setUpTPU():\n    \n    # Detect hardware, return appropriate distribution strategy\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n        strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\n    print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n    return strategy\n\n\n\ndef format_path(st):\n    #return \"../input/plant-pathology-2020-fgvc7/images/\" + st + '.jpg'\n    return GCS_DS_PATH + '/images/' + st + '.jpg'\n\n\n\ndef format_paths():\n    train_paths = train['image_id'].apply(format_path).values\n    test_paths = test['image_id'].apply(format_path).values\n    train_labels = train.loc[:, 'healthy':].values\n    return train_paths,test_paths,train_labels\n\n\n\n\n### Learning rate schedule - adapted from https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models\ndef lrfn(epoch):\n    \n    lr_start=0.00001\n    lr_max=0.0001 * strategy.num_replicas_in_sync # maybe change this\n    lr_min=0.00001\n    lr_rampup_epochs=8\n    lr_sustain_epochs=3\n    lr_exp_decay=.8\n    \n    if epoch < lr_rampup_epochs:\n        lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n    elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n        lr = lr_max\n    else:\n        lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n    return lr\n\n\n\n### Plotting the learning rate schedule\ndef plot_lr():\n    rng = [i for i in range(EPOCHS)]\n    y = [lrfn(x) for x in rng]\n    plt.plot(rng, y)\n    print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\n\n\n\ndef print_count(df,string):\n    temp = df[df[string] == 1]\n    print(string,'column has', temp.shape[0], 'values')\n\n\n### EfficientNet Model\ndef getModel1(): \n    with strategy.scope():\n        model = tf.keras.Sequential([\n            efn.EfficientNetB7(\n                input_shape=(img_size_y, img_size_x, 3),\n                weights='imagenet',\n                include_top=False\n            ),\n            L.GlobalAveragePooling2D(),\n            L.Dense(train_labels.shape[1], activation='softmax')\n        ])\n    \n        model.compile(\n            optimizer='adam',\n            loss = 'categorical_crossentropy',\n            metrics=['categorical_accuracy']\n        )\n    \n        return model\n\n\n### Inception ResNet Model\ndef getModel2():\n    with strategy.scope():\n        model = tf.keras.Sequential([\n            InceptionResNetV2(\n                input_shape=(img_size_y, img_size_x, 3),\n                weights='imagenet',\n                include_top=False\n            ),\n            L.GlobalAveragePooling2D(),\n            L.Dense(train_labels.shape[1], activation='softmax')\n        ])\n    \n        model.compile(\n            optimizer='adam',\n            loss = 'categorical_crossentropy',\n            metrics=['categorical_accuracy']\n        )\n    \n        return model\n\n\n### DenseNet Model\ndef getModel3():\n    with strategy.scope():\n        model = tf.keras.Sequential([\n            DenseNet201(\n                input_shape=(img_size_y, img_size_x, 3),\n                weights='imagenet',\n                include_top=False\n            ),\n            L.GlobalAveragePooling2D(),\n            L.Dense(train_labels.shape[1], activation='softmax')\n        ])\n    \n        model.compile(\n            optimizer='adam',\n            loss = 'categorical_crossentropy',\n            metrics=['categorical_accuracy']\n        )\n    \n        return model\n\n\n### Xception Model\ndef getModel4():\n    with strategy.scope():\n        model = tf.keras.Sequential([\n            Xception(\n                input_shape=(img_size_y, img_size_x, 3),\n                weights='imagenet',\n                include_top=False\n            ),\n            L.GlobalAveragePooling2D(),\n            L.Dense(train_labels.shape[1], activation='softmax')\n        ])\n    \n        model.compile(\n            optimizer='adam',\n            loss = 'categorical_crossentropy',\n            metrics=['categorical_accuracy']\n        )\n    \n        return model\n\n    \n### ResNet50 Model\ndef getModel5():\n    with strategy.scope():\n        model = tf.keras.Sequential([\n            ResNet50V2(\n                input_shape=(img_size_y, img_size_x, 3),\n                weights='imagenet',\n                include_top=False\n            ),\n            L.GlobalAveragePooling2D(),\n            L.Dense(train_labels.shape[1], activation='softmax')\n        ])\n    \n        model.compile(\n            optimizer='adam',\n            loss = 'categorical_crossentropy',\n            metrics=['categorical_accuracy']\n        )\n    \n        return model\n\n\n### Splits data, grabs model, fits, and then computes and returns predictions\ndef modelFit(i,train_paths,test_paths,train_labels):\n    \n    if kfolding == False:\n        \n        train_paths, valid_paths, train_labels, valid_labels = train_test_split(train_paths, train_labels, test_size=TEST_SIZE, random_state=SEED)\n        \n        class_weights = compute_class_weight('balanced',np.unique(train_labels.argmax(axis=1)),train_labels.argmax(axis=1))\n        print('class weights: ',class_weights)\n        \n        train_dataset,valid_dataset,test_dataset = get_datasets(train_paths,valid_paths,train_labels,valid_labels)\n        \n        if (i == 0):\n            model = getModel1()\n        if (i == 1):\n            model = getModel2()\n        if (i == 2):\n            model = getModel3()\n        if (i == 3):\n            model = getModel4()\n        if (i == 4):\n            model = getModel5()\n        \n        model.summary()\n        \n        STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n        \n        history = model.fit(\n            train_dataset,\n            epochs=EPOCHS,\n            verbose=2,\n            callbacks=[lr_schedule],\n            steps_per_epoch = STEPS_PER_EPOCH,\n            validation_data=valid_dataset,\n            class_weight=class_weights\n        )\n        \n        final_predictions = model.predict(test_dataset)\n        \n        \n    if kfolding == True:\n    \n        predictions = []\n        \n        for train_index,test_index in KFold(FOLDS,shuffle=True,random_state=SEED).split(train_paths):\n        \n            print('FOLDING')\n        \n            train_paths_new, valid_paths = train_paths[train_index],train_paths[test_index]\n            train_labels_new, valid_labels= train_labels[train_index],train_labels[test_index]\n            \n            class_weights = compute_class_weight('balanced',np.unique(train_labels_new.argmax(axis=1)),train_labels_new.argmax(axis=1))\n            print('class weights: ',class_weights)\n            \n            train_dataset,valid_dataset,test_dataset = get_datasets(train_paths_new,valid_paths,train_labels_new,valid_labels)\n            \n            if (i == 0):\n                model = getModel1() \n            if (i == 1):\n                model = getModel2() \n            if (i == 2):\n                model = getModel3() \n            if (i == 3):\n                model = getModel4() \n            if (i == 4):\n                model = getModel5() \n    \n            \n            STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n        \n            history = model.fit(\n                train_dataset,\n                epochs=EPOCHS,\n                verbose=2,\n                callbacks=[lr_schedule],\n                steps_per_epoch = STEPS_PER_EPOCH,\n                validation_data=valid_dataset,\n                class_weight = class_weights\n            )\n        \n            preds = model.predict(test_dataset)\n        \n            print(preds[0])\n        \n            predictions.append(preds)\n    \n        final_predictions = np.mean(predictions,axis=0)\n        \n    return final_predictions\n\n    \n### Gotten from https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models\ndef decode_image(filename, label=None, image_size=(img_size_y, img_size_x)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\n\n### Gotten from https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\n\n\n### Construct datasets - adapted from https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models\ndef get_datasets(train_paths,valid_paths,train_labels,valid_labels):\n\n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((train_paths, train_labels))\n        .map(decode_image, num_parallel_calls=AUTO)\n        .cache()\n        .map(data_augment, num_parallel_calls=AUTO)\n        .repeat()\n        .shuffle(512)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n\n    valid_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((valid_paths, valid_labels))\n        .map(decode_image, num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE)\n        .cache()\n        .prefetch(AUTO)\n    )\n\n    test_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices(test_paths)\n        .map(decode_image, num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE)\n    )\n    \n    return train_dataset,valid_dataset,test_dataset\n    \n\n    \n    \n    \n    \n    \n\n    \n    \n\n### Data access and other starting settings\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\nAUTO = tf.data.experimental.AUTOTUNE\n\nstrategy = setUpTPU()\n    \nBATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\n\n\n\n\n\n\n### Read and store data\ntrain = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/train.csv')\ntest = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/test.csv')\nsub = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv')\n\n\n\n\n\nprint('Total Train Data Counts:')\nprint_count(train,'healthy')\nprint_count(train,'multiple_diseases')\nprint_count(train,'rust')\nprint_count(train,'scab')\n\n\n\n\n\n\n#plot_lr() # Plot what the learning rate function looks like\n\n\n\n\nprint('Formatting Paths...')\n\ntrain_paths,test_paths,train_labels = format_paths() \n\n\n\n\n\n\n\n\n# set up learning rate schedule\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n\n\n\n\n\n# go through different models and get predictions\n\npredictions = []\n\nfor i in range(number_of_models):\n    \n    preds = modelFit(i,train_paths,test_paths,train_labels)\n\n    predictions.append(preds)\n\n\n\n\n\n\nfinal_predictions = np.mean(predictions,axis=0)\n\n\n\n\n### Submit results\nsub.loc[:, 'healthy':] = final_predictions\nsub.to_csv('submission.csv', index=False)\nsub.head()\n\n\n\n\n","execution_count":2,"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: You are using pip version 20.1; however, version 20.1.1 is available.\r\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n","name":"stdout"},{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"Running on TPU  grpc://10.0.0.2:8470\nNumber of devices: 8\nTotal Train Data Counts:\nhealthy column has 516 values\nmultiple_diseases column has 91 values\nrust column has 622 values\nscab column has 592 values\nFormatting Paths...\nFOLDING\nclass weights:  [0.88349515 4.91891892 0.73833671 0.76310273]\nDownloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b7_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n258441216/258434480 [==============================] - 3s 0us/step\nTrain for 7 steps, validate for 2 steps\n\nEpoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\nEpoch 1/5\n7/7 - 338s - loss: 1.3973 - categorical_accuracy: 0.2606 - val_loss: 1.4761 - val_categorical_accuracy: 0.0795\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 0.00010875.\nEpoch 2/5\n7/7 - 3s - loss: 1.3175 - categorical_accuracy: 0.4051 - val_loss: 1.3512 - val_categorical_accuracy: 0.2712\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 0.0002075.\nEpoch 3/5\n7/7 - 3s - loss: 1.0498 - categorical_accuracy: 0.6629 - val_loss: 0.9639 - val_categorical_accuracy: 0.7178\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 0.00030625000000000004.\nEpoch 4/5\n7/7 - 3s - loss: 0.5104 - categorical_accuracy: 0.8588 - val_loss: 0.8498 - val_categorical_accuracy: 0.7699\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 0.00040500000000000003.\nEpoch 5/5\n7/7 - 3s - loss: 0.2941 - categorical_accuracy: 0.9118 - val_loss: 1.0017 - val_categorical_accuracy: 0.7973\n[2.7151732e-08 2.2816193e-05 9.9997711e-01 4.0435555e-08]\nFOLDING\nclass weights:  [0.86315166 5.875      0.74034553 0.75727651]\nTrain for 7 steps, validate for 2 steps\n\nEpoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\nEpoch 1/5\n7/7 - 285s - loss: 1.3989 - categorical_accuracy: 0.2640 - val_loss: 1.3306 - val_categorical_accuracy: 0.3681\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 0.00010875.\nEpoch 2/5\n7/7 - 3s - loss: 1.3102 - categorical_accuracy: 0.4051 - val_loss: 1.2833 - val_categorical_accuracy: 0.4203\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 0.0002075.\nEpoch 3/5\n7/7 - 3s - loss: 1.0663 - categorical_accuracy: 0.6579 - val_loss: 1.0865 - val_categorical_accuracy: 0.5467\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 0.00030625000000000004.\nEpoch 4/5\n7/7 - 3s - loss: 0.5107 - categorical_accuracy: 0.8544 - val_loss: 0.9009 - val_categorical_accuracy: 0.6511\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 0.00040500000000000003.\nEpoch 5/5\n7/7 - 3s - loss: 0.2886 - categorical_accuracy: 0.9124 - val_loss: 1.1471 - val_categorical_accuracy: 0.7253\n[8.5381288e-08 1.6714117e-05 9.9998307e-01 1.2029794e-07]\nFOLDING\nclass weights:  [0.88841463 4.98972603 0.73884381 0.75727651]\nTrain for 7 steps, validate for 2 steps\n\nEpoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\nEpoch 1/5\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-c66a5fbc220e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_of_models\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelFit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_paths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_paths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-c66a5fbc220e>\u001b[0m in \u001b[0;36mmodelFit\u001b[0;34m(i, train_paths, test_paths, train_labels)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0mclass_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m             )\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    820\u001b[0m   \"\"\"\n\u001b[1;32m    821\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \"\"\"\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}