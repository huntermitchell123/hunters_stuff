{"cells":[{"cell_type":"code","source":"# Hunter Mitchell - 5/26/20 - Plant Pathology 2020 Competition Code\n\n# This code uses TPU and an ensemble of multiple deep learning models to classify the diseases on images of apple leaves\n\n\n\n!pip install -q efficientnet\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nimport cv2\n\nimport tensorflow.keras.layers as L\nimport efficientnet.tfkeras as efn\nfrom tensorflow.keras.applications import DenseNet201,InceptionResNetV2,Xception,ResNet50V2\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom kaggle_datasets import KaggleDatasets\n\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.utils.class_weight import compute_class_weight\n\n\n\n\n\n\n\n### SETTINGS ###\n\n# Image Size (Original images are 1365 x 2048)\nimg_size_x = 512\nimg_size_y = 512\n\n# Cross Validation Settings\nkfolding = False\nFOLDS = 5 \n\n# Splitting Settings\nTEST_SIZE = .15 \nSEED = 2020\n\n# How many different models to ensemble (up to 5)\nnumber_of_models = 1\n\n# Model Settings\nBATCH_SIZE = 8\nEPOCHS = 35\n\n\n\n\n\n\n\n### TPU Stuff\ndef setUpTPU():\n    \n    # Detect hardware, return appropriate distribution strategy\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n        strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\n    print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n    return strategy\n\n\n\ndef format_path(st):\n    #return \"../input/plant-pathology-2020-fgvc7/images/\" + st + '.jpg'\n    return GCS_DS_PATH + '/images/' + st + '.jpg'\n\n\n\ndef format_paths():\n    train_paths = train['image_id'].apply(format_path).values\n    test_paths = test['image_id'].apply(format_path).values\n    train_labels = train.loc[:, 'healthy':].values\n    return train_paths,test_paths,train_labels\n\n\n\n\n### Learning rate schedule - adapted from https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models\ndef lrfn(epoch):\n    \n    lr_start=0.00001\n    lr_max=0.0001 * strategy.num_replicas_in_sync # maybe change this\n    lr_min=0.00001\n    lr_rampup_epochs=8\n    lr_sustain_epochs=3\n    lr_exp_decay=.8\n    \n    if epoch < lr_rampup_epochs:\n        lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n    elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n        lr = lr_max\n    else:\n        lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n    return lr\n\n\n\n### Plotting the learning rate schedule\ndef plot_lr():\n    rng = [i for i in range(EPOCHS)]\n    y = [lrfn(x) for x in rng]\n    plt.plot(rng, y)\n    print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\n\n\n\ndef print_count(df,string):\n    temp = df[df[string] == 1]\n    print(string,'column has', temp.shape[0], 'values')\n\n\n### EfficientNet Model\ndef getModel1(): \n    with strategy.scope():\n        model = tf.keras.Sequential([\n            efn.EfficientNetB7(\n                input_shape=(img_size_y, img_size_x, 3),\n                weights='imagenet',\n                include_top=False\n            ),\n            L.GlobalAveragePooling2D(),\n            L.Dense(train_labels.shape[1], activation='softmax')\n        ])\n    \n        model.compile(\n            optimizer='adam',\n            loss = 'categorical_crossentropy',\n            metrics=['categorical_accuracy']\n        )\n    \n        return model\n\n\n### Inception ResNet Model\ndef getModel2():\n    with strategy.scope():\n        model = tf.keras.Sequential([\n            InceptionResNetV2(\n                input_shape=(img_size_y, img_size_x, 3),\n                weights='imagenet',\n                include_top=False\n            ),\n            L.GlobalAveragePooling2D(),\n            L.Dense(train_labels.shape[1], activation='softmax')\n        ])\n    \n        model.compile(\n            optimizer='adam',\n            loss = 'categorical_crossentropy',\n            metrics=['categorical_accuracy']\n        )\n    \n        return model\n\n\n### DenseNet Model\ndef getModel3():\n    with strategy.scope():\n        model = tf.keras.Sequential([\n            DenseNet201(\n                input_shape=(img_size_y, img_size_x, 3),\n                weights='imagenet',\n                include_top=False\n            ),\n            L.GlobalAveragePooling2D(),\n            L.Dense(train_labels.shape[1], activation='softmax')\n        ])\n    \n        model.compile(\n            optimizer='adam',\n            loss = 'categorical_crossentropy',\n            metrics=['categorical_accuracy']\n        )\n    \n        return model\n\n\n### Xception Model\ndef getModel4():\n    with strategy.scope():\n        model = tf.keras.Sequential([\n            Xception(\n                input_shape=(img_size_y, img_size_x, 3),\n                weights='imagenet',\n                include_top=False\n            ),\n            L.GlobalAveragePooling2D(),\n            L.Dense(train_labels.shape[1], activation='softmax')\n        ])\n    \n        model.compile(\n            optimizer='adam',\n            loss = 'categorical_crossentropy',\n            metrics=['categorical_accuracy']\n        )\n    \n        return model\n\n    \n### ResNet50 Model\ndef getModel5():\n    with strategy.scope():\n        model = tf.keras.Sequential([\n            ResNet50V2(\n                input_shape=(img_size_y, img_size_x, 3),\n                weights='imagenet',\n                include_top=False\n            ),\n            L.GlobalAveragePooling2D(),\n            L.Dense(train_labels.shape[1], activation='softmax')\n        ])\n    \n        model.compile(\n            optimizer='adam',\n            loss = 'categorical_crossentropy',\n            metrics=['categorical_accuracy']\n        )\n    \n        return model\n\n\n### Splits data, grabs model, fits, and then computes and returns predictions\ndef modelFit(i,train_paths,test_paths,train_labels):\n    \n    if kfolding == False:\n        \n        train_paths, valid_paths, train_labels, valid_labels = train_test_split(train_paths, train_labels, test_size=TEST_SIZE, random_state=SEED)\n        \n        class_weights = compute_class_weight('balanced',np.unique(train_labels.argmax(axis=1)),train_labels.argmax(axis=1))\n        print('class weights: ',class_weights)\n        \n        train_dataset,valid_dataset,test_dataset = get_datasets(train_paths,valid_paths,train_labels,valid_labels)\n        \n        if (i == 0):\n            model = getModel1()\n        if (i == 1):\n            model = getModel2()\n        if (i == 2):\n            model = getModel3()\n        if (i == 3):\n            model = getModel4()\n        if (i == 4):\n            model = getModel5()\n        \n        model.summary()\n        \n        STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n        \n        history = model.fit(\n            train_dataset,\n            epochs=EPOCHS,\n            verbose=2,\n            callbacks=[lr_schedule],\n            steps_per_epoch = STEPS_PER_EPOCH,\n            validation_data=valid_dataset,\n            class_weight=class_weights\n        )\n        \n        final_predictions = model.predict(test_dataset)\n        \n        \n    if kfolding == True:\n    \n        predictions = []\n        \n        for train_index,test_index in KFold(FOLDS,shuffle=True,random_state=SEED).split(train_paths):\n        \n            print('FOLDING')\n        \n            train_paths_new, valid_paths = train_paths[train_index],train_paths[test_index]\n            train_labels_new, valid_labels= train_labels[train_index],train_labels[test_index]\n            \n            class_weights = compute_class_weight('balanced',np.unique(train_labels_new.argmax(axis=1)),train_labels_new.argmax(axis=1))\n            print('class weights: ',class_weights)\n            \n            train_dataset,valid_dataset,test_dataset = get_datasets(train_paths_new,valid_paths,train_labels_new,valid_labels)\n            \n            if (i == 0):\n                model = getModel1() \n            if (i == 1):\n                model = getModel2() \n            if (i == 2):\n                model = getModel3() \n            if (i == 3):\n                model = getModel4() \n            if (i == 4):\n                model = getModel5() \n    \n            \n            STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n        \n            history = model.fit(\n                train_dataset,\n                epochs=EPOCHS,\n                verbose=2,\n                callbacks=[lr_schedule],\n                steps_per_epoch = STEPS_PER_EPOCH,\n                validation_data=valid_dataset,\n                class_weight = class_weights\n            )\n        \n            preds = model.predict(test_dataset)\n        \n            print(preds[0])\n        \n            predictions.append(preds)\n    \n        final_predictions = np.mean(predictions,axis=0)\n        \n    return final_predictions\n\n    \n### Gotten from https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models\ndef decode_image(filename, label=None, image_size=(img_size_y, img_size_x)):\n    bits = tf.io.read_file(filename)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) / 255.0\n    image = tf.image.resize(image, image_size)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\n\n### Gotten from https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models\ndef data_augment(image, label=None):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\n\n\n### Construct datasets - adapted from https://www.kaggle.com/tarunpaparaju/plant-pathology-2020-eda-models\ndef get_datasets(train_paths,valid_paths,train_labels,valid_labels):\n\n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((train_paths, train_labels))\n        .map(decode_image, num_parallel_calls=AUTO)\n        .cache()\n        .map(data_augment, num_parallel_calls=AUTO)\n        .repeat()\n        .shuffle(512)\n        .batch(BATCH_SIZE)\n        .prefetch(AUTO)\n    )\n\n    valid_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((valid_paths, valid_labels))\n        .map(decode_image, num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE)\n        .cache()\n        .prefetch(AUTO)\n    )\n\n    test_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices(test_paths)\n        .map(decode_image, num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE)\n    )\n    \n    return train_dataset,valid_dataset,test_dataset\n    \n\n    \n    \n    \n    \n    \n\n    \n    \n\n### Data access and other starting settings\nGCS_DS_PATH = KaggleDatasets().get_gcs_path()\n\nAUTO = tf.data.experimental.AUTOTUNE\n\nstrategy = setUpTPU()\n    \nBATCH_SIZE = BATCH_SIZE * strategy.num_replicas_in_sync\n\n\n\n\n\n\n### Read and store data\ntrain = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/train.csv')\ntest = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/test.csv')\nsub = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv')\n\n\n\n\n\nprint('Total Train Data Counts:')\nprint_count(train,'healthy')\nprint_count(train,'multiple_diseases')\nprint_count(train,'rust')\nprint_count(train,'scab')\n\n\n\n\n\n\n#plot_lr() # Plot what the learning rate function looks like\n\n\n\n\nprint('Formatting Paths...')\n\ntrain_paths,test_paths,train_labels = format_paths() \n\n\n\n\n\n\n\n\n# set up learning rate schedule\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n\n\n\n\n\n# go through different models and get predictions\n\npredictions = []\n\nfor i in range(number_of_models):\n    \n    preds = modelFit(i,train_paths,test_paths,train_labels)\n\n    predictions.append(preds)\n\n\n\n\n\n\nfinal_predictions = np.mean(predictions,axis=0)\n\n\n\n\n### Submit results\nsub.loc[:, 'healthy':] = final_predictions\nsub.to_csv('submission.csv', index=False)\nsub.head()\n\n\n\n\n","metadata":{"collapsed":false,"_kg_hide-input":false},"execution_count":0,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":4}